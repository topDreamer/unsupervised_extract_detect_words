1.思路：借鉴之前有一篇blog，利用人人网数据进行新词挖掘的思想，做了改进优化；

2.原始思路： 利用jieba对文档分词，3个相邻词为一组，计算两个词的左信息熵，右信息熵，内部的凝聚度，并据此进行计算分数，根据分数大小获取新词；

3.优化点： 1.针对只能结合两个词，泛化到结合计算相邻N个词；

          2.内部互信息【凝聚度计算】，归一化到长度=1个词的情况下的值，可以实现不同长度词在同一纬度下进行比较；
          
          3.多进程处理，提高运行速度；
          
          4.添加过滤机制，根据停用词,高频常用词等进行过滤
          
4.入口文件： segment_multi.py   

  执行方式： python segment_multi.py


效果展示：
  ('_重大_疾病', 0.017789747314352424)
  
('_保障_范围', 0.015639743403053734)

('_本_公司', 0.014212133249451173)

('_完全_丧失', 0.013672071599779227)

('_满足_下列', 0.01222551433926759)

('_不_在', 0.011979165549614371)

('_在_保障_范围', 0.011447420114413524)

('_不_在_保障_范围', 0.011437253239670606)

('_意外_伤害', 0.010722245979224557)

('_保障_范围_内', 0.01060853796282469)

('_在_保障', 0.010412326193951542)

('_范围_内', 0.009957964710051568)

('_不_在_保障', 0.009778776893266231)

('_不_在_保障_范围_内', 0.009252386106707824)

('_明确_诊断', 0.009062853195861094)

('_日常生活_活动', 0.008990786509666062)

('_六项_基本_日常生活', 0.008813957372202039)

('_基本_日常生活', 0.008694797110512052)

('_基本_日常生活_活动', 0.008671016020472998)

('_保险_事故', 0.008504469334120192)

('_六项_基本_日常生活_活动', 0.008471400808888209)

